{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926d3f4a-b355-4931-ad78-302bc6446d47",
   "metadata": {},
   "outputs": [],
   "source": [
    " import gym\n",
    " import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d1c2bc-2c86-4589-95f0-71887729c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",render_mode=\"human\",is_slippery=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67307900-e19e-442f-b3f5-3966ba339f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9509900498999999, 0.96059601, 0.9702989999999999, 0.96059601, 0.96059601, 0.0, 0.9801, 0.0, 0.9702989999999999, 0.9801, 0.99, 0.0, 0.0, 0.99, 1.0, 0.0]\n",
      "[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]\n",
      "(4, 0.0, False, False, {'prob': 1.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 0.0, False, False, {'prob': 1.0})\n",
      "(9, 0.0, False, False, {'prob': 1.0})\n",
      "(13, 0.0, False, False, {'prob': 1.0})\n",
      "(14, 0.0, False, False, {'prob': 1.0})\n",
      "(15, 1.0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "(15, 0, True, False, {'prob': 1.0})\n",
      "le nombre de succes est : 10\n"
     ]
    }
   ],
   "source": [
    "class ValueIteration():\n",
    "    \n",
    "    def __init__(self,env,n_iteration,discount_facture):\n",
    "        self.num_state=env.observation_space.n\n",
    "        self.num_action = env.action_space.n\n",
    "        self.n_iteration=n_iteration\n",
    "        self.discount_facture=discount_facture\n",
    "        self.value_table = [0] * self.num_state\n",
    "        self.optimal_policy = [0]*self.num_state\n",
    "\n",
    "    \n",
    "    def get_optimal_value(self):\n",
    "        for i in range(self.n_iteration):\n",
    "            for state in range(self.num_state):\n",
    "                action_Q =[0]* self.num_action\n",
    "                for action in range(self.num_action):\n",
    "                     info_state = env.P[state][action]\n",
    "                     #[(0.33,2,1),(0.33,1,0),(probabilite,new_state,reward obtenu)]\n",
    "                     probabilite = np.array([x[0] for x in info_state])\n",
    "\n",
    "                     R  =np.array([x[2] for x in info_state]) + self.discount_facture * np.array([self.value_table[x[1]] for x in info_state])\n",
    "                     Q=sum(probabilite*R)\n",
    "                     action_Q[action] = Q\n",
    "                self.value_table[state]=max(action_Q)\n",
    "        print(self.value_table)\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        for state in range(self.num_state):\n",
    "            action_Q =[0]*self.num_action\n",
    "            for action in range(self.num_action):\n",
    "                    info_state = env.P[state][action]\n",
    "                     #[(0.33,2,1),(0.33,1,0),(probabilite,new_state,reward obtenu)]\n",
    "                    probabilite = np.array([x[0] for x in info_state])\n",
    "\n",
    "                    R  =np.array([x[2] for x in info_state]) + self.discount_facture * np.array([self.value_table[x[1]] for x in info_state])\n",
    "                    Q=sum(probabilite*R)\n",
    "                    action_Q[action] = Q\n",
    "            self.optimal_policy[state]=np.argmax(action_Q)\n",
    "        return self.optimal_policy\n",
    "\n",
    "\n",
    "cur_state = env.reset()\n",
    "n_iteration =10000\n",
    "discount = 0.99\n",
    "app=ValueIteration(env,n_iteration,discount)\n",
    "app.get_optimal_value()\n",
    "\n",
    "policy =app.get_optimal_policy()\n",
    "print(policy)\n",
    "\n",
    "s=0\n",
    "done=False\n",
    "count = 0 \n",
    "for episode in range(10):\n",
    "  s=0\n",
    "  done=False\n",
    "  while not done:\n",
    "     t=env.step(int(policy[s]))\n",
    "     print(t)\n",
    "     s=int(t[0])\n",
    "     done = t[2] or t[3]\n",
    "     if t[2]==True : \n",
    "         count +=1\n",
    "     env.render()\n",
    "print(f\"le nombre de succes est : {count}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc84029-3eab-4a58-b0ee-3f7503736ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1e57e-2914-4e4c-b22d-bff14383af71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea19ffc-51df-4c0a-a7ed-a45cbd1e516c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
